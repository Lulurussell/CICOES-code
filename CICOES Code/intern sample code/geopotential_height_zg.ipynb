{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This code is for 3 calculations related to the geopotential height (variable name zg). We use the Pangeo CMIP6 Public Dataset. A lot of this code was modified starting from code from the Pangeo Gallery,found [here](https://gallery.pangeo.io/repos/pangeo-gallery/cmip6/).\n",
    "\n",
    "Additionally, the [xarray documentation](https://xarray.pydata.org/en/stable/), [xarray-extras documentation](https://xarray-extras.readthedocs.io/en/latest/), [seaborn](https://seaborn.pydata.org), [matplotlib](https://matplotlib.org/) along with the online documentation for any of the other used libraries are a useful resource.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This first block of code is explained in time_slice.py."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "from xarray_extras import csv\n",
    "import numpy as np\n",
    "import dask\n",
    "from dask.diagnostics import progress\n",
    "from tqdm.autonotebook import tqdm\n",
    "import intake\n",
    "import fsspec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "import cftime\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "\n",
    "col = intake.open_esm_datastore(\"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\")\n",
    "\n",
    "expts = ['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585', 'piControl']\n",
    "\n",
    "\n",
    "query = dict(\n",
    "    experiment_id = expts,\n",
    "    variable_id = ['zg'],\n",
    "    table_id = ['Amon'],\n",
    "    source_id = ['BCC-CSM2-MR'],\n",
    "    member_id = 'r1i1p1f1'\n",
    ")\n",
    "col_subset = col.search(**query)\n",
    "\n",
    "# print(col_subset.df.groupby(\"source_id\")[\n",
    "#     [\"experiment_id\", \"variable_id\", \"table_id\"]\n",
    "# ].nunique())\n",
    "\n",
    "def drop_all_bounds(ds):\n",
    "    \"\"\"Drop coordinates like 'time_bounds' from datasets,\n",
    "    which can lead to issues when merging.\"\"\"\n",
    "    drop_vars = [vname for vname in ds.coords if (('_bounds') in vname ) or ('_bnds') in vname]\n",
    "    return ds.drop(drop_vars)\n",
    "\n",
    "\n",
    "def open_dsets(df):\n",
    "    \"\"\"Open datasets from cloud storage and return xarray dataset.\"\"\"\n",
    "    dsets = [xr.open_zarr(fsspec.get_mapper(ds_url), consolidated=True).pipe(drop_all_bounds) for ds_url in df.zstore]\n",
    "    try:\n",
    "        ds = xr.merge(dsets, join='exact')\n",
    "        return ds\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def open_delayed(df):\n",
    "    \"\"\"A dask.delayed wrapper around `open_dsets`.\n",
    "    Allows us to open many datasets in parallel.\"\"\"\n",
    "    return dask.delayed(open_dsets)(df)\n",
    "\n",
    "\n",
    "dsets_aligned = {}\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "dsets = defaultdict(dict)\n",
    "for group, df in col_subset.df.groupby(by=['source_id', 'experiment_id']):\n",
    "\n",
    "    dsets[group[0]][group[1]] = open_delayed(df)\n",
    "\n",
    "open_dsets(df)\n",
    "dsets_ = dask.compute(dict(dsets))[0]\n",
    "\n",
    "expt_da = xr.DataArray(expts, dims='experiment_id', name='experiment_id', coords={'experiment_id': expts})\n",
    "\n",
    "dsets_aligned = {}\n"
   ]
  },
  {
   "source": [
    "Our next block is a set of 3 functions having to do with the [Pacific/North American teleconnection pattern](https://www.cpc.ncep.noaa.gov/data/teledoc/pna.shtml).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(ls, a):\n",
    "    \"\"\"Returns closest number to a in list ls\"\"\"\n",
    "    # we need this function becuase we don't actually get the exact coordinates we desire from our data. \n",
    "    # We can also make this function more sophisiticated by limitting it to only output\n",
    "    # if within an acceptable range.\n",
    "\n",
    "    return min(ls, key=lambda x:abs(x-a))\n",
    "\n",
    "def PNA_prelim(ds, lat, lon):\n",
    "    \"\"\"Return data for zg at lat, lon with elevation at 50000\"\"\"\n",
    "    # We call float() on closest so that .sel() doesn't get mad at us.\n",
    "    return ds.sel(plev=50000, lon=float(closest(list(ds.sel(plev=50000)['lon']), lon)), lat=float(closest(list(ds.sel(plev=50000)['lat']), lat)))\n",
    "def PNA(ds):\n",
    "\n",
    "    # lat, lon = 20 N, 160 W\n",
    "    a = PNA_prelim(ds, 20, 205)\n",
    "    # lat, lon = 45 N, 165 W\n",
    "    b = PNA_prelim(ds, 45, 200)\n",
    "\n",
    "    #lat, lon = 55 N, 115 W\n",
    "    c = PNA_prelim(ds, 55, 250)\n",
    "\n",
    "    #lat, lon = 30 N, 85 W\n",
    "    d = PNA_prelim(ds, 30, 280)\n",
    "\n",
    "    return 0.25*(a - b + c - d).rename({'zg':'PNA'})"
   ]
  },
  {
   "source": [
    "Next, [GBI](https://psl.noaa.gov/data/timeseries/daily/GBI/). While the function is only really one line of code, it's a big complicated to look at. A more aesthetically pleasing way to look at it is as follows:\n",
    "```\n",
    "def GBI(ds):\n",
    "    return ds.sel(\n",
    "            plev=50000, \n",
    "            lat=slice(\n",
    "                float(closest(list(ds.sel(plev=50000)['lat']), 60)),\n",
    "                float(closest(list(ds.sel(plev=50000)['lat']), 80))\n",
    "            ), \n",
    "            lon=slice(\n",
    "                float(closest(list(ds.sel(plev=50000)['lon']), 280)), \n",
    "                float(closest(list(ds.sel(plev=50000)['lon']), 340))\n",
    "            )\n",
    "        ).mean(dim='lat').mean(dim='lon').rename({'zg':'GBI'})\n",
    "```\n",
    "We select the relevant ranges for latitude and longitude, then take the mean across them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBI(ds):\n",
    "    return ds.sel(plev=50000, lat=slice(float(closest(list(ds.sel(plev=50000)['lat']), 60)), float(closest(list(ds.sel(plev=50000)['lat']), 80))), lon=slice(float(closest(list(ds.sel(plev=50000)['lon']), 280)), float(closest(list(ds.sel(plev=50000)['lon']), 340)))).mean(dim='lat').mean(dim='lon').rename({'zg':'GBI'})"
   ]
  },
  {
   "source": [
    "And finally, PCA:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(ds):\n",
    "    return ds.sel(lat=slice(float(closest(list(ds['lat']), 65)),\n",
    "        float(closest(list(ds['lat']), 90)))).mean(dim='lat').mean(dim='lon').rename({'zg':'PCA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in tqdm(dsets_.items()):\n",
    "    expt_dsets = v.values()\n",
    "    if any([d is None for d in expt_dsets]):\n",
    "        print(f\"Missing experiment for {k}\")\n",
    "        continue\n",
    "    \n",
    "    # Uncomment if plotting\n",
    "    # Add a 'year' axis\n",
    "    # for ds in expt_dsets:\n",
    "    #     ds.coords['year'] = ds.time.dt.year\n",
    "\n",
    "    # workaround for\n",
    "    # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n",
    "    # Remove 'time' dimension for year, and coarsen the 'year' axis such that it is actually yearly data, as opposed to monthly\n",
    "    # dsets_ann_mean = [v[expt].pipe(PCA)\n",
    "    #                   for expt in expts]\n",
    "    # #                          .swap_dims({'time': 'year'})\n",
    "    # #                          .drop('time') \n",
    "    # #                          .coarsen(year=12).mean()\n",
    "    # dsets_aligned[k] = xr.concat(dsets_ann_mean, join='outer',\n",
    "    #                              dim=expt_da)\n",
    "\n",
    "    # Write PCA\n",
    "    for expt in expts:\n",
    "        # Check if the experiment actually exists, skip iteration if not.\n",
    "        try:\n",
    "            a = v[expt]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        # Remember to replace file locaiton with correct one when running!\n",
    "        print(f\"writing data to /run/media/jqiu21/Elements/Jason/Internship/PCA/mon/PCA_{k}_{expt}.nc\")\n",
    "        with progress.ProgressBar():\n",
    "            x = PCA(v[expt]).compute().sel(time=slice('2015-01-01', '2100-12-31'))\n",
    "            x.to_netcdf(path=f\"/run/media/jqiu21/Elements/Jason/Internship/PCA/mon/PCA_{k}_{expt}.nc\", mode='w',  engine='netcdf4', format='NETCDF4')\n",
    "\n",
    "    #Write PNA\n",
    "    for expt in expts:\n",
    "        try:\n",
    "            a = v[expt]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        print(f\"writing data to /run/media/jqiu21/Elements/Jason/Internship/PNA/mon/PNA_{k}_{expt}.nc\")\n",
    "        with progress.ProgressBar():\n",
    "            x = PNA(v[expt]).compute().sel(time=slice('2015-01-01', '2100-12-31'))\n",
    "        x.to_netcdf(path=f\"/run/media/jqiu21/Elements/Jason/Internship/PNA/mon/PNA_{k}_{expt}.nc\", mode='w',  engine='netcdf4', format='NETCDF4')\n",
    "    #Write GBI\n",
    "    for expt in expts:\n",
    "        try:\n",
    "            a = v[expt]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        print(f\"writing data to /run/media/jqiu21/Elements/Jason/Internship/GBI/mon/GBI_{k}_{expt}.nc\")\n",
    "        with progress.ProgressBar():\n",
    "            x = GBI(v[expt]).compute().sel(time=slice('2015-01-01', '2100-12-31'))\n",
    "            x.to_netcdf(path=f\"/run/media/jqiu21/Elements/Jason/Internship/GBI/mon/GBI_{k}_{expt}.nc\", mode='w',  engine='netcdf4', format='NETCDF4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code is only for plotting. Uncomment the code in the previous block before running this block.\n",
    "# Concatenate all datasets (from multiple sources) into one\n",
    "\n",
    "with progress.ProgressBar():\n",
    "    dsets_aligned_ = dask.compute(dsets_aligned)[0]\n",
    "\n",
    "source_ids = list(dsets_aligned_.keys())\n",
    "source_da = xr.DataArray(source_ids, dims='source_id', name='source_id', coords={'source_id': source_ids})\n",
    "\n",
    "big_ds = xr.concat([ds.reset_coords(drop=True)\n",
    "                    for ds in dsets_aligned_.values()],\n",
    "                    dim=source_da)\n",
    "\n",
    "Create dataframe to plot from\n",
    "df_all = big_ds.sel(year=slice(1950, 2100)).to_dataframe().reset_index()\n",
    "print(df_all.head())\n",
    "\n",
    "Actually plot the data\n",
    "sns.set()\n",
    "\n",
    "# If plotting GBI or PNA, change y label\n",
    "p = sns.relplot(data=df_all,\n",
    "            x=\"year\", y=\"PCA\", hue='experiment_id',\n",
    "            kind=\"line\", ci=\"sd\", size=10, aspect=2, legend=False)\n",
    "\n",
    "I had some issues with random '9's or '10's in the automatically generated legend, so I made my own.\n",
    "plt.legend(title='experiment_id', loc='best', labels=expts)\n",
    "\n",
    "plt.show()"
   ]
  }
 ]
}