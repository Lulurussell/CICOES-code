{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to take the arctic global temperature and produce a graph of it across time. We use the Pangeo CMIP6 Public Dataset. A lot of this code was modified starting from code from the Pangeo Gallery,found [here](https://gallery.pangeo.io/repos/pangeo-gallery/cmip6/).\n",
    "\n",
    "Additionally, the [xarray documentation](https://xarray.pydata.org/en/stable/), [xarray-extras documentation](https://xarray-extras.readthedocs.io/en/latest/), [seaborn](https://seaborn.pydata.org), [matplotlib](https://matplotlib.org/) along with the online documentation for any of the other used libraries are a useful resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first block of code is covered in more detail in the file time_slice.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask\n",
    "from dask.diagnostics import progress\n",
    "from tqdm.autonotebook import tqdm\n",
    "import intake\n",
    "import fsspec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "\n",
    "\n",
    "# df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "# print(df.head())\n",
    "col = intake.open_esm_datastore(\"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\")\n",
    "\n",
    "# print([eid for eid in col.df['experiment_id'].unique() if 'ssp' in eid])\n",
    "\n",
    "\n",
    "expts = ['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "\n",
    "query = dict(\n",
    "    experiment_id = expts,\n",
    "\n",
    "    variable_id = ['tas'],\n",
    "    table_id = ['Amon'],\n",
    "    source_id = ['ACCESS-ESM1-5','CESM2-WACCM', 'MPI-ESM1-2-HR'],\n",
    "    member_id = 'r1i1p1f1'\n",
    ")\n",
    "col_subset = col.search(require_all_on=[\"source_id\"], **query)\n",
    "# col_subset.df.groupby(\"source_id\")[\n",
    "#     [\"experiment_id\", \"variable_id\", \"table_id\"]\n",
    "# ].nunique()\n",
    "\n",
    "def drop_all_bounds(ds):\n",
    "    \"\"\"Drop coordinates like 'time_bounds' from datasets,\n",
    "    which can lead to issues when merging.\"\"\"\n",
    "    drop_vars = [vname for vname in ds.coords if (('_bounds') in vname ) or ('_bnds') in vname]\n",
    "    return ds.drop(drop_vars)\n",
    "\n",
    "def open_dsets(df):\n",
    "    \"\"\"Open datasets from cloud storage and return xarray dataset.\"\"\"\n",
    "    dsets = [xr.open_zarr(fsspec.get_mapper(ds_url), consolidated=True).pipe(drop_all_bounds) for ds_url in df.zstore]\n",
    "    try:\n",
    "        ds = xr.merge(dsets, join='exact')\n",
    "        return ds\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def open_delayed(df):\n",
    "    \"\"\"A dask.delayed wrapper around `open_dsets`.\n",
    "    Allows us to open many datasets in parallel.\"\"\"\n",
    "    return dask.delayed(open_dsets)(df)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "dsets = defaultdict(dict)\n",
    "for group, df in col_subset.df.groupby(by=['source_id', 'experiment_id']):\n",
    "    dsets[group[0]][group[1]] = open_delayed(df)\n",
    "\n",
    "open_dsets(df)\n",
    "dsets_ = dask.compute(dict(dsets))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate the global and arctic mean. It seems like there should be an easier method using this [example](https://xarray.pydata.org/en/stable/examples/area_weighted_temperature.html) from xarray's documentation. However, I only found this after finished this bit of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_name(ds):\n",
    "    \"\"\"Get what the dimension name for latitude is (either 'lat' or 'latitude'\"\"\"\n",
    "    for lat_name in ['lat', 'latitude']:\n",
    "        if lat_name in ds.coords:\n",
    "            return lat_name\n",
    "    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n",
    "def modified_weight(lat):\n",
    "    \"\"\"modified version of the cosine function whose domain only covers values greater than 60 degrees\"\"\"\n",
    "    return xr.where(lat > 60, np.cos(np.deg2rad(lat)), 0)\n",
    "def global_mean(ds):\n",
    "    \"\"\"Take the global temperature mean across latitude and longitude\"\"\"\n",
    "    # To take the mean across time, you can simply use the .mean() method.\n",
    "    lat = ds[get_lat_name(ds)]\n",
    "    weight = np.cos(np.deg2rad(lat))\n",
    "    weight /= weight.mean()\n",
    "    other_dims = set(ds.dims) - {'time'} \n",
    "    return (ds * weight).mean(other_dims) # We take the weighted mean across all other dimensions other than time.\n",
    "\n",
    "def arctic_mean(ds):\n",
    "    \"\"\"Modified version of global mean - only considers temperatures in the arctic circle\"\"\"\n",
    "    lat = ds[get_lat_name(ds)]\n",
    "    weight = modified_weight(lat)\n",
    "    weight /= weight.mean()\n",
    "    other_dims = set(ds.dims) - {'time'}\n",
    "    return (ds * weight).mean(other_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block is preparing our dsets_ dictionary of datasets to be converted into a dataframe, which is the data type we need for seaborn to plot the global mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e5b703d8ab4341b0885d412594706d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'expt_da' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4ad72af92cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# align everything with the 4xCO2 experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     dsets_aligned[k] = xr.concat(dsets_ann_mean, join='outer',\n\u001b[0;32m---> 20\u001b[0;31m                                  dim=expt_da)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'expt_da' is not defined"
     ]
    }
   ],
   "source": [
    "for k, v in tqdm(dsets_.items()):\n",
    "    expt_dsets = v.values()\n",
    "    if any([d is None for d in expt_dsets]):\n",
    "        print(f\"Missing experiment for {k}\")\n",
    "        continue\n",
    "\n",
    "    for ds in expt_dsets:\n",
    "        ds.coords['year'] = ds.time.dt.year\n",
    "\n",
    "    # workaround for\n",
    "    # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n",
    "    dsets_ann_mean = [v[expt].pipe(arctic_mean)\n",
    "                             .swap_dims({'time': 'year'})\n",
    "                             .drop('time')\n",
    "                             .coarsen(year=12).mean()\n",
    "                      for expt in expts]\n",
    "\n",
    "    # align everything with the 4xCO2 experiment\n",
    "    dsets_aligned[k] = xr.concat(dsets_ann_mean, join='outer',\n",
    "                                 dim=expt_da)\n",
    "\n",
    "with progress.ProgressBar():\n",
    "    dsets_aligned_ = dask.compute(dsets_aligned)[0]\n",
    "\n",
    "source_ids = list(dsets_aligned_.keys())\n",
    "source_da = xr.DataArray(source_ids, dims='source_id', name='source_id', coords={'source_id': source_ids})\n",
    "\n",
    "big_ds = xr.concat([ds.reset_coords(drop=True)\n",
    "                    for ds in dsets_aligned_.values()],\n",
    "                    dim=source_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = big_ds.sel(year=slice(1900, 2100)).to_dataframe().reset_index()\n",
    "\n",
    "sns.set()\n",
    "p = sns.relplot(data=df_all,\n",
    "            x=\"year\", y=\"tas\", hue='experiment_id',\n",
    "            kind=\"line\", ci=\"sd\", size=10, aspect=2, legend=False)\n",
    "\n",
    "# We create our own legend because I found that seaborn's legend often randomly contained other nonexistant lines. This is why we imported matplotlib separately from seaborn.\n",
    "plt.legend(title='experiment_id', loc='best', labels=['historical', 'ssp126', 'ssp245', 'ssp370', 'ssp585'])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
