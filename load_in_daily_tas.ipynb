{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-d0d7acd7e5d4>:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask\n",
    "from dask.diagnostics import progress\n",
    "from tqdm.autonotebook import tqdm \n",
    "import intake\n",
    "import fsspec\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = intake.open_esm_datastore(\"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ssp370SST-ssp126Lu',\n",
       " 'ssp370SST-lowNTCF',\n",
       " 'ssp370SST-lowCH4',\n",
       " 'ssp370-lowNTCF',\n",
       " 'ssp370pdSST',\n",
       " 'ssp370SST',\n",
       " 'ssp245',\n",
       " 'esm-ssp585',\n",
       " 'ssp126-ssp370Lu',\n",
       " 'ssp370-ssp126Lu',\n",
       " 'esm-ssp585-ssp126Lu',\n",
       " 'ssp585',\n",
       " 'ssp370',\n",
       " 'ssp126',\n",
       " 'ssp119',\n",
       " 'ssp245-GHG',\n",
       " 'ssp245-nat',\n",
       " 'ssp434',\n",
       " 'ssp460',\n",
       " 'ssp534-over',\n",
       " 'ssp245-aer',\n",
       " 'ssp245-stratO3',\n",
       " 'ssp245-cov-fossil',\n",
       " 'ssp245-covid',\n",
       " 'ssp245-cov-strgreen',\n",
       " 'ssp245-cov-modgreen',\n",
       " 'ssp585-bgc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[eid for eid in col.df['experiment_id'].unique() if 'ssp' in eid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>variable_id</th>\n",
       "      <th>table_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACCESS-CM2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACCESS-ESM1-5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWI-CM-1-1-MR</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BCC-CSM2-MR</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CESM2-WACCM</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMCC-CM2-SR5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMCC-ESM2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CanESM5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EC-Earth3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EC-Earth3-Veg</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EC-Earth3-Veg-LR</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FGOALS-g3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GFDL-ESM4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IITM-ESM</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INM-CM4-8</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INM-CM5-0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IPSL-CM6A-LR</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KACE-1-0-G</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KIOST-ESM</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIROC6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPI-ESM1-2-HR</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPI-ESM1-2-LR</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRI-ESM2-0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NESM3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NorESM2-LM</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NorESM2-MM</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  experiment_id  variable_id  table_id\n",
       "source_id                                             \n",
       "ACCESS-CM2                    4            1         1\n",
       "ACCESS-ESM1-5                 4            1         1\n",
       "AWI-CM-1-1-MR                 4            1         1\n",
       "BCC-CSM2-MR                   4            1         1\n",
       "CESM2-WACCM                   4            1         1\n",
       "CMCC-CM2-SR5                  4            1         1\n",
       "CMCC-ESM2                     4            1         1\n",
       "CanESM5                       4            1         1\n",
       "EC-Earth3                     4            1         1\n",
       "EC-Earth3-Veg                 4            1         1\n",
       "EC-Earth3-Veg-LR              4            1         1\n",
       "FGOALS-g3                     4            1         1\n",
       "GFDL-ESM4                     4            1         1\n",
       "IITM-ESM                      4            1         1\n",
       "INM-CM4-8                     4            1         1\n",
       "INM-CM5-0                     4            1         1\n",
       "IPSL-CM6A-LR                  4            1         1\n",
       "KACE-1-0-G                    4            1         1\n",
       "KIOST-ESM                     4            1         1\n",
       "MIROC6                        4            1         1\n",
       "MPI-ESM1-2-HR                 4            1         1\n",
       "MPI-ESM1-2-LR                 4            1         1\n",
       "MRI-ESM2-0                    4            1         1\n",
       "NESM3                         4            1         1\n",
       "NorESM2-LM                    4            1         1\n",
       "NorESM2-MM                    4            1         1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is currently a significant amount of data for these runs\n",
    "#expts = ['historical', 'ssp245', 'ssp585']\n",
    "expts = ['historical','ssp126','ssp245','ssp585']\n",
    "query = dict(\n",
    "    experiment_id=expts,\n",
    "    table_id='day',                           \n",
    "    variable_id=['tas'],\n",
    "    member_id = 'r1i1p1f1',                     \n",
    ")\n",
    "\n",
    "col_subset = col.search(require_all_on=[\"source_id\"], **query)\n",
    "col_subset.df.groupby(\"source_id\")[\n",
    "    [\"experiment_id\", \"variable_id\", \"table_id\"]\n",
    "].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = intake.open_esm_datastore('/Volumes/Transcend/tas_Amon_CAS-ESM2-0_ssp245_r1i1p1f1_gn_201501-210012.nc')\n",
    "def drop_all_bounds(ds):\n",
    "    drop_vars = [vname for vname in ds.coords\n",
    "                 if (('_bounds') in vname ) or ('_bnds') in vname]\n",
    "    return ds.drop(drop_vars)\n",
    "\n",
    "def open_dset(df):\n",
    "    assert len(df) == 1\n",
    "    ds = xr.open_zarr(fsspec.get_mapper(df.zstore.values[0]), consolidated=True)\n",
    "    return drop_all_bounds(ds)\n",
    "\n",
    "def open_delayed(df):\n",
    "    return dask.delayed(open_dset)(df)\n",
    "\n",
    "from collections import defaultdict\n",
    "dsets = defaultdict(dict) \n",
    "\n",
    "for group, df in col_subset.df.groupby(by=['source_id', 'experiment_id']):\n",
    "    dsets[group[0]][group[1]] = open_delayed(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lulur/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/coding/times.py:527: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/home/lulur/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/coding/times.py:527: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/home/lulur/.conda/envs/cicoes39/lib/python3.9/site-packages/numpy/core/_asarray.py:102: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/home/lulur/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/coding/times.py:527: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/home/lulur/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/coding/times.py:527: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/home/lulur/.conda/envs/cicoes39/lib/python3.9/site-packages/numpy/core/_asarray.py:102: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/home/lulur/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/coding/times.py:527: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/home/lulur/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/coding/times.py:527: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/home/lulur/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/coding/times.py:527: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/home/lulur/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/coding/times.py:527: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/home/lulur/.conda/envs/cicoes39/lib/python3.9/site-packages/numpy/core/_asarray.py:102: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/home/lulur/.conda/envs/cicoes39/lib/python3.9/site-packages/numpy/core/_asarray.py:102: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "dsets_ = dask.compute(dict(dsets))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate global means\n",
    "\n",
    "def get_lat_name(ds):\n",
    "    for lat_name in ['lat', 'latitude']:\n",
    "        if lat_name in ds.coords:\n",
    "            return lat_name\n",
    "    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n",
    "\n",
    "def global_mean(ds):\n",
    "    lat = ds[get_lat_name(ds)]\n",
    "    weight = np.cos(np.deg2rad(lat))\n",
    "    weight /= weight.mean()\n",
    "    other_dims = set(ds.dims) - {'time'}\n",
    "    return (ds * weight).mean(other_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k, v in tqdm(dsets_.items()):\n",
    "\n",
    "    # Check for any missing experiments that we expect\n",
    "    expt_dsets = v.values()\n",
    "    if any([d is None for d in expt_dsets]):\n",
    "        print(f\"Missing experiment for {k}\")\n",
    "        continue\n",
    "\n",
    "    climatology = v['historical'].sel(time=slice('1981-01-01', '2010-12-31')).groupby('time.month').mean('time')\n",
    "\n",
    "    for i in v:\n",
    "        if i == 'historical':\n",
    "            # When working with daily data, it's very easy for objects to run your computer out of memory, so we shorten historical runs because to the date range we need.\n",
    "            anomaly = v[i].sel(time=slice('1981-01-01', '2010-12-31')).groupby('time.month') - climatology\n",
    "        else:\n",
    "            anomaly = v[i].groupby('time.month') - climatology\n",
    "        # Because these files are too large to store in memory, we use the option compute=False to create a dask delayed object and then compute it later.\n",
    "        # Remember to change the file location to the relevant folder.\n",
    "        delayed_obj = anomaly.to_netcdf(path=f\"/nobackup/users/lulur/tas-anomaly_{k}_{i}.nc\", mode='w', compute=False, engine='netcdf4', format='NETCDF4')\n",
    "        print(f\"writing data to /nobackup/users/lulur/tas-anomaly_{k}_{i}.nc\")\n",
    "\n",
    "        with progress.ProgressBar():\n",
    "            results = delayed_obj.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing data to /nobackup/users/lulur/tas-anomaly_ACCESS-ESM1-5_historical.nc\n",
      "[########################################] | 100% Completed | 27.3s\n",
      "writing data to /nobackup/users/lulur/tas-anomaly_ACCESS-ESM1-5_ssp126.nc\n",
      "[########################################] | 100% Completed |  3min  6.9s\n",
      "writing data to /nobackup/users/lulur/tas-anomaly_ACCESS-ESM1-5_ssp245.nc\n",
      "[########################################] | 100% Completed |  3min 40.2s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to update size for existing dimension'time' (104459 != 10957)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fb9a84452470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Because these files are too large to store in memory, we use the option compute=False to create a dask delayed object and then compute it later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Remember to change the file location to the relevant folder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdelayed_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manomaly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"/nobackup/users/lulur/tas-anomaly_{k}_{i}.nc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'netcdf4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NETCDF4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"writing data to /nobackup/users/lulur/tas-anomaly_{k}_{i}.nc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1797\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_netcdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1799\u001b[0;31m         return to_netcdf(\n\u001b[0m\u001b[1;32m   1800\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0;31m# TODO: allow this work (setting up the file for writing array data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;31m# to be parallelized with dask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m         dump_to_store(\n\u001b[0m\u001b[1;32m   1077\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m         )\n",
      "\u001b[0;32m~/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mdump_to_store\u001b[0;34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m     \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/backends/common.py\u001b[0m in \u001b[0;36mstore\u001b[0;34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         self.set_variables(\n\u001b[1;32m    267\u001b[0m             \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_encoding_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cicoes39/lib/python3.9/site-packages/xarray/backends/common.py\u001b[0m in \u001b[0;36mset_dimensions\u001b[0;34m(self, variables, unlimited_dims)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_dims\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexisting_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    337\u001b[0m                     \u001b[0;34m\"Unable to update size for existing dimension\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0;34mf\"{dim!r} ({length} != {existing_dims[dim]})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to update size for existing dimension'time' (104459 != 10957)"
     ]
    }
   ],
   "source": [
    "#for a SPECIFIC model, use the cell below:\n",
    "# Check for any missing experiments that we expect\n",
    "models = ['ACCESS-ESM1-5','NorESM2-LM','NorESM2-MM']\n",
    "for k in models:\n",
    "#k = 'ACCESS-WACCM'\n",
    "    v = dsets_[k]\n",
    "    expt_dsets = v.values()\n",
    "    #if any([d is None for d in expt_dsets]):\n",
    "        #print(f\"Missing experiment for {k}\")\n",
    "        #continue\n",
    "\n",
    "    climatology = v['historical'].sel(time=slice('1981-01-01', '2010-12-31')).groupby('time.dayofyear').mean('time')\n",
    "\n",
    "    for i in v:\n",
    "        if i == 'historical':\n",
    "            # When working with daily data, it's very easy for objects to run your computer out of memory, so we shorten historical runs because to the date range we need.\n",
    "            anomaly = v[i].sel(time=slice('1981-01-01', '2010-12-31')).groupby('time.dayofyear') - climatology\n",
    "        else:\n",
    "            anomaly = v[i].groupby('time.dayofyear') - climatology\n",
    "        # Because these files are too large to store in memory, we use the option compute=False to create a dask delayed object and then compute it later.\n",
    "        # Remember to change the file location to the relevant folder.\n",
    "        delayed_obj = anomaly.to_netcdf(path=f\"/nobackup/users/lulur/tas-anomaly_{k}_{i}.nc\", mode='w', compute=False, engine='netcdf4', format='NETCDF4')\n",
    "        print(f\"writing data to /nobackup/users/lulur/tas-anomaly_{k}_{i}.nc\")\n",
    "\n",
    "        with progress.ProgressBar():\n",
    "            results = delayed_obj.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_da = xr.DataArray(expts, dims='experiment_id', name='experiment_id',\n",
    "                        coords={'experiment_id': expts})\n",
    "\n",
    "dsets_aligned = {}\n",
    "\n",
    "for k, v in tqdm(dsets_.items()):\n",
    "    expt_dsets = v.values()\n",
    "    if any([d is None for d in expt_dsets]):\n",
    "        print(f\"Missing experiment for {k}\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    print(type(v[expt]) for expt in expts)\n",
    "    for ds in expt_dsets:\n",
    "        #ds.coords['year'] = ds.time.dt.year\n",
    "        #ds.coords['month']=ds.time.dt.month\n",
    "        #ds.coords['rawday'] = cftime.date2num\n",
    "        ds.coords['rawmonth']=ds.time.dt.month+(ds.time.dt.year-1850)*12\n",
    "        #print(ds.coords)\n",
    "    # workaround for\n",
    "    # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n",
    "    dsets_mon_mean = [v[expt].pipe(global_mean)\n",
    "                             .swap_dims({'time': 'rawmonth'})\n",
    "                             .drop('time')\n",
    "                             #.drop('mon')\n",
    "    #                         .coarsen(year=12).mean()\n",
    "                      for expt in expts]\n",
    "    \n",
    "    #print(dsets_mon_mean)\n",
    "    \n",
    "    # align everything with the 4xCO2 experiment\n",
    "    dsets_aligned[k] = xr.concat(dsets_mon_mean, join='outer',\n",
    "                                 dim=expt_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expt_da = xr.DataArray(expts, dims='experiment_id', name='experiment_id',\n",
    "#                         coords={'experiment_id': expts})\n",
    "\n",
    "# dsets_aligned = {}\n",
    "\n",
    "# for k, v in tqdm(dsets_.items()):\n",
    "#     expt_dsets = v.values()\n",
    "#     if any([d is None for d in expt_dsets]):\n",
    "#         print(f\"Missing experiment for {k}\")\n",
    "#         continue\n",
    "    \n",
    "#     for ds in expt_dsets:\n",
    "#         #ds.coords['year'] = ds.time.dt.year\n",
    "#         #ds.coords['month']=ds.time.dt.month\n",
    "#         ds.coords['rawmonth']=ds.time.dt.month+(ds.time.dt.year-1850)*12\n",
    "#         #print(ds.coords)\n",
    "#     # workaround for\n",
    "#     # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n",
    "#     dsets_mon_mean = [v[expt].pipe(global_mean)\n",
    "#                              .swap_dims({'time': 'rawmonth'})\n",
    "#                              .drop('time')\n",
    "#                              #.drop('mon')\n",
    "#     #                         .coarsen(year=12).mean()\n",
    "#                       for expt in expts]\n",
    "    \n",
    "#     #print(dsets_mon_mean)\n",
    "    \n",
    "#     # align everything with the 4xCO2 experiment\n",
    "#     dsets_aligned[k] = xr.concat(dsets_mon_mean, join='outer',\n",
    "#                                  dim=expt_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expt_da = xr.DataArray(expts, dims='experiment_id', name='experiment_id',\n",
    "#                        coords={'experiment_id': expts})\n",
    "\n",
    "# dsets_aligned = {}\n",
    "\n",
    "# for k, v in tqdm(dsets_.items()):\n",
    "#     expt_dsets = v.values()\n",
    "#     if any([d is None for d in expt_dsets]):\n",
    "#         print(f\"Missing experiment for {k}\")\n",
    "#         continue\n",
    "    \n",
    "#     for ds in expt_dsets:\n",
    "#         ds.coords['year'] = ds.time.dt.year\n",
    "        \n",
    "#     # workaround for\n",
    "#     # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n",
    "#     dsets_ann_mean = [v[expt].pipe(global_mean)\n",
    "#                              .swap_dims({'time': 'year'})\n",
    "#                              .drop('time')\n",
    "#                              .coarsen(year=12).mean()\n",
    "#                       for expt in expts]\n",
    "    \n",
    "#     # align everything with the 4xCO2 experiment\n",
    "#     dsets_aligned[k] = xr.concat(dsets_ann_mean, join='outer',\n",
    "#                                  dim=expt_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with progress.ProgressBar():\n",
    "    dsets_aligned_ = dask.compute(dsets_aligned)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = list(dsets_aligned_.keys())\n",
    "source_da = xr.DataArray(source_ids, dims='source_id', name='source_id',\n",
    "                         coords={'source_id': source_ids})\n",
    "\n",
    "big_ds = xr.concat([ds.reset_coords(drop=True)\n",
    "                    for ds in dsets_aligned_.values()],\n",
    "                    dim=source_da)\n",
    "\n",
    "big_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = big_ds.sel(year=slice(1900, 2100)).to_dataframe().reset_index()\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=df_all,\n",
    "            x=\"year\", y=\"tas\", hue='experiment_id',\n",
    "            kind=\"line\", ci=\"sd\", aspect=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all.shape)\n",
    "df_by_exp = df_all.groupby('experiment_id')\n",
    "\n",
    "tas_exp = []\n",
    "for name, group in df_by_exp:\n",
    "    group_mon = group.groupby('rawmonth')\n",
    "    tas = np.empty((13,251))\n",
    "    tas[0,:]=np.arange(1850,2101)\n",
    "    month = []\n",
    "    year = []\n",
    "    for mon, mongroup in group_mon:\n",
    "        if mon>251:\n",
    "            break\n",
    "        montemp = np.nanmean(mongroup['tas'])\n",
    "        i_month = int(mon%12)\n",
    "        i_year = int(np.floor(mon/12))\n",
    "        tas[i_month,i_year] = montemp\n",
    "    #print(np.count_nonzero(tas))\n",
    "    #print(np.count_nonzero(np.isnan(tas)))\n",
    "    tas_exp.append(tas)\n",
    "    #years = (np.asarray(months)/12)+1850\n",
    "    #plt.plot(years,avg_temps,label=name,alpha=0.6)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_all.shape)\n",
    "# df_by_exp = df_all.groupby('experiment_id')\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# for name, group in df_by_exp:\n",
    "#     group_mon = group.groupby('rawmonth')\n",
    "#     avg_temps = []\n",
    "#     months = []\n",
    "#     for mon, mongroup in group_mon:\n",
    "#         montemp = np.nanmean(mongroup['tas'])\n",
    "#         avg_temps.append(montemp)\n",
    "#         months.append(mon)\n",
    "#     temps = np.array(group['tas'])\n",
    "#     print(temps.shape)\n",
    "#     print(group.shape)\n",
    "#     years = (np.asarray(months)/12)+1850\n",
    "#     plt.plot(years,avg_temps,label=name,alpha=0.6)\n",
    "#     #plt.show()\n",
    "# #plt.xlim(2000,2020)\n",
    "# #plt.ylim(270,296)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all['year'].shape)\n",
    "#years = np.array(df_all['year'])\n",
    "#print(years.shape)\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(years,'b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "for name, group in df_by_exp:\n",
    "    for \n",
    "    plt.plot(group['year'],label=name)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cicoes39",
   "language": "python",
   "name": "cicoes39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
